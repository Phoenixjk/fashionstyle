import argparse, os, sys, globimport PILimport torchimport torch.nn as nnimport numpy as npfrom omegaconf import OmegaConffrom datetime import datetimefrom PIL import Imagefrom tqdm import tqdm, trangefrom itertools import islicefrom DBlearning import contrastive_lossfrom einops import rearrange, repeatfrom torchvision.utils import make_gridfrom torch import autocastfrom contextlib import nullcontextimport timefrom pytorch_lightning import seed_everythingimport torchimport torchvision.models as modelsimport torchvision.transforms as transformsfrom PIL import Imageimport osfrom torch.utils.data import DataLoaderfrom datasets import ImageDatasetfrom CrossAttention import CrossAttentionimport torch.nn.functional as Ffrom idea import extract_clothing_shapesys.path.append(os.path.dirname(sys.path[0]))from ldm.util import instantiate_from_configfrom ldm.models.diffusion.ddim import DDIMSamplerfrom ldm.models.diffusion.plms import PLMSSamplerfrom transformers import CLIPProcessor, CLIPModelclass MyModel(nn.Module):    def __init__(self,device):        super(MyModel, self).__init__()        self.linear_layer = nn.Sequential(            nn.Conv1d(512, 77, kernel_size=1),            nn.Linear(256, 768)        ).to(device)        # self.dim_map = nn.Conv1d(77*2, 77, kernel_size=1)        self.dim_map = nn.Conv1d(77, 77, kernel_size=1)        # self.cross_attention = CrossAttention(query_dim=neirong_features_flat.size(2), context_dim=fengge_features_flat.size(2),        #                                  heads=8, dim_head=64, dropout=0.).to(device)        self.cross_attention = CrossAttention(query_dim=768, context_dim=768,heads=8, dim_head=64, dropout=0.).to(device)    def forward(self, neirong_features_flat, fengge_features_flat):        neirong_features_flat = self.linear_layer(neirong_features_flat)  #[1,512,256]-> [1,77,768]        fengge_features_flat = self.linear_layer(fengge_features_flat)        output_features1 = self.cross_attention(neirong_features_flat, fengge_features_flat).float()  # 交叉注意力输出的融合特征   [8,512,256]        # output_features2 = self.cross_attention(fengge_features_flat, neirong_features_flat)  # 交叉注意力输出的融合特征   [8,512,256]        # output_features = torch.concatenate([output_features1, output_features2], dim=1)        output_features = self.dim_map(output_features1).float()        # output_features = self.dim_map(output_features)        return output_features